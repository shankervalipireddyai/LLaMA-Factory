{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shankervalipireddyai/LLaMA-Factory/blob/main/Making_the_Most_of_your_Colab_Subscription.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQ4bH7qMGrA"
      },
      "source": [
        "# Making the Most of your Colab Subscription\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMMqmdiYMkvi"
      },
      "source": [
        "## Faster GPUs\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to premium GPUs. You can upgrade your notebook's GPU settings in `Runtime > Change runtime type` in the menu to enable Premium accelerator. Subject to availability, selecting a premium GPU may grant you access to a V100 or A100 Nvidia GPU.\n",
        "\n",
        "The free of charge version of Colab grants access to Nvidia's T4 GPUs subject to quota restrictions and availability.\n",
        "\n",
        "You can see what GPU you've been assigned at any time by executing the following cell. If the execution result of running the code cell below is \"Not connected to a GPU\", you can change the runtime by going to `Runtime > Change runtime type` in the menu to enable a GPU accelerator, and then re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23TOba33L4qf",
        "outputId": "e1f2bcb4-b4dd-48d3-d7e7-b0375db48daa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Mar 27 06:58:53 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0              26W / 300W |      0MiB / 16384MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/shankervalipireddyai/LLaMA-Factory.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4CkkoeTtLBx",
        "outputId": "5278612d-759c-4cc7-a91d-7546fb6e0000"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LLaMA-Factory'...\n",
            "remote: Enumerating objects: 8500, done.\u001b[K\n",
            "remote: Counting objects: 100% (2285/2285), done.\u001b[K\n",
            "remote: Compressing objects: 100% (420/420), done.\u001b[K\n",
            "remote: Total 8500 (delta 2050), reused 1952 (delta 1859), pack-reused 6215\u001b[K\n",
            "Receiving objects: 100% (8500/8500), 216.66 MiB | 31.60 MiB/s, done.\n",
            "Resolving deltas: 100% (6227/6227), done.\n",
            "Updating files: 100% (190/190), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd LLaMA-Factory\n",
        "!pip install -r requirements.txt\n",
        "!CUDA CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iW2lZyotUVt",
        "outputId": "aef8f7e0-e50c-42a0-e5fc-976339b5436d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/LLaMA-Factory\n",
            "Requirement already satisfied: torch>=1.13.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.37.2 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (4.38.2)\n",
            "Collecting datasets>=2.14.3 (from -r requirements.txt (line 3))\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate>=0.27.2 (from -r requirements.txt (line 4))\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft>=0.9.0 (from -r requirements.txt (line 5))\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl>=0.8.1 (from -r requirements.txt (line 6))\n",
            "  Downloading trl-0.8.1-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.0/225.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting gradio<4.0.0,>=3.38.0 (from -r requirements.txt (line 7))\n",
            "  Downloading gradio-3.50.2-py3-none-any.whl (20.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.3/20.3 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (1.11.4)\n",
            "Collecting einops (from -r requirements.txt (line 9))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (0.1.99)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (3.20.3)\n",
            "Collecting uvicorn (from -r requirements.txt (line 12))\n",
            "  Downloading uvicorn-0.29.0-py3-none-any.whl (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (2.6.4)\n",
            "Collecting fastapi (from -r requirements.txt (line 14))\n",
            "  Downloading fastapi-0.110.0-py3-none-any.whl (92 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.1/92.1 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sse-starlette (from -r requirements.txt (line 15))\n",
            "  Downloading sse_starlette-2.0.0-py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (3.7.1)\n",
            "Collecting fire (from -r requirements.txt (line 17))\n",
            "  Downloading fire-0.6.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.4/88.4 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting galore-torch (from -r requirements.txt (line 18))\n",
            "  Downloading galore_torch-1.0-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m70.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.1->-r requirements.txt (line 1)) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.1->-r requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.37.2->-r requirements.txt (line 2)) (4.66.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets>=2.14.3->-r requirements.txt (line 3))\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (1.5.3)\n",
            "Collecting xxhash (from datasets>=2.14.3->-r requirements.txt (line 3))\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets>=2.14.3->-r requirements.txt (line 3))\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.14.3->-r requirements.txt (line 3)) (3.9.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.27.2->-r requirements.txt (line 4)) (5.9.5)\n",
            "Collecting tyro>=0.5.11 (from trl>=0.8.1->-r requirements.txt (line 6))\n",
            "  Downloading tyro-0.7.3-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.8/79.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiofiles<24.0,>=22.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.2.2)\n",
            "Collecting ffmpy (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.6.1 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading gradio_client-0.6.1-py3-none-any.whl (299 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.2/299.2 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (6.3.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (9.4.0)\n",
            "Collecting pydub (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting websockets<12.0,>=10.0 (from gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn->-r requirements.txt (line 12)) (8.1.7)\n",
            "Collecting h11>=0.8 (from uvicorn->-r requirements.txt (line 12))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 13)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic->-r requirements.txt (line 13)) (2.16.3)\n",
            "Collecting starlette<0.37.0,>=0.36.3 (from fastapi->-r requirements.txt (line 14))\n",
            "  Downloading starlette-0.36.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from sse-starlette->-r requirements.txt (line 15)) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (4.50.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->-r requirements.txt (line 16)) (2.8.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 17)) (1.16.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->-r requirements.txt (line 17)) (2.4.0)\n",
            "Collecting bitsandbytes (from galore-torch->-r requirements.txt (line 18))\n",
            "  Downloading bitsandbytes-0.43.0-py3-none-manylinux_2_24_x86_64.whl (102.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.12.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.14.3->-r requirements.txt (line 3)) (4.0.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets>=2.14.3->-r requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.37.2->-r requirements.txt (line 2)) (2024.2.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->sse-starlette->-r requirements.txt (line 15)) (1.2.0)\n",
            "Collecting docstring-parser>=0.14.1 (from tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6))\n",
            "  Downloading docstring_parser-0.16-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6)) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6))\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Collecting httpcore==1.* (from httpx->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7))\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.1->-r requirements.txt (line 1)) (1.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.34.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio<4.0.0,>=3.38.0->-r requirements.txt (line 7)) (0.18.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl>=0.8.1->-r requirements.txt (line 6)) (0.1.2)\n",
            "Building wheels for collected packages: fire, ffmpy\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.6.0-py2.py3-none-any.whl size=117029 sha256=3338a9d021e94609085dc8ff376784fef089ff8cada0976c68d1d6c6dda15a6e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/6d/5d/5b73fa0f46d01a793713f8859201361e9e581ced8c75e5c6a3\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=34b1d8b9491cc4b02a29eac9e4a54cefc24cddd7a578854b22bf1d8e43a259e9\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "Successfully built fire ffmpy\n",
            "Installing collected packages: pydub, ffmpy, xxhash, websockets, shtab, semantic-version, python-multipart, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, h11, fire, einops, docstring-parser, dill, aiofiles, uvicorn, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, httpcore, tyro, sse-starlette, nvidia-cusolver-cu12, httpx, fastapi, gradio-client, datasets, gradio, bitsandbytes, accelerate, trl, peft, galore-torch\n",
            "Successfully installed accelerate-0.28.0 aiofiles-23.2.1 bitsandbytes-0.43.0 datasets-2.18.0 dill-0.3.8 docstring-parser-0.16 einops-0.7.0 fastapi-0.110.0 ffmpy-0.3.2 fire-0.6.0 galore-torch-1.0 gradio-3.50.2 gradio-client-0.6.1 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 multiprocess-0.70.16 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105 orjson-3.9.15 peft-0.10.0 pydub-0.25.1 python-multipart-0.0.9 semantic-version-2.10.0 shtab-1.7.1 sse-starlette-2.0.0 starlette-0.36.3 trl-0.8.1 tyro-0.7.3 uvicorn-0.29.0 websockets-11.0.3 xxhash-3.4.1\n",
            "/bin/bash: line 1: CUDA: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ToAgfzcQ4qfc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!CUDA_VISIBLE_DEVICES=0 python src/train_web.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CtQUEQk-vjtp",
        "outputId": "2d9076a6-848a-499f-ff96-7a1c478558b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2024-03-27 08:00:08.833573: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-03-27 08:00:08.833639: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-03-27 08:00:08.835263: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-03-27 08:00:09.871662: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://1f31ddb3205eeb4044.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Exception in thread Thread-6 (run_exp):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n",
            "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 94, in get_train_args\n",
            "    model_args, data_args, training_args, finetuning_args, generating_args = _parse_train_args(args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 80, in _parse_train_args\n",
            "    return _parse_args(parser, args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/hparams/parser.py\", line 39, in _parse_args\n",
            "    return parser.parse_dict(args)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/hf_argparser.py\", line 373, in parse_dict\n",
            "    obj = dtype(**inputs)\n",
            "  File \"<string>\", line 128, in __init__\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py\", line 1482, in __post_init__\n",
            "    raise ValueError(\n",
            "ValueError: Your setup doesn't support bf16/gpu. You need torch>=1.10, using Ampere GPU with cuda>=11.0\n",
            "03/27/2024 08:07:43 - WARNING - llmtuner.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
            "03/27/2024 08:07:43 - INFO - llmtuner.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, compute dtype: torch.float16\n",
            "tokenizer_config.json: 100% 1.47k/1.47k [00:00<00:00, 7.70MB/s]\n",
            "tokenizer.model: 100% 493k/493k [00:00<00:00, 18.6MB/s]\n",
            "special_tokens_map.json: 100% 72.0/72.0 [00:00<00:00, 362kB/s]\n",
            "tokenizer.json: 100% 1.80M/1.80M [00:00<00:00, 5.70MB/s]\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-03-27 08:07:45,172 >> loading file tokenizer.model from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.model\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-03-27 08:07:45,172 >> loading file added_tokens.json from cache at None\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-03-27 08:07:45,172 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-03-27 08:07:45,172 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2046] 2024-03-27 08:07:45,172 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/tokenizer.json\n",
            "03/27/2024 08:07:45 - INFO - llmtuner.data.template - Add pad token: </s>\n",
            "03/27/2024 08:07:45 - INFO - llmtuner.data.loader - Loading dataset MattCoddity/dockerNLcommands...\n",
            "Downloading readme: 100% 1.46k/1.46k [00:00<00:00, 9.09MB/s]\n",
            "Downloading data: 100% 543k/543k [00:00<00:00, 1.60MB/s]\n",
            "Generating train split: 2415 examples [00:00, 87864.16 examples/s]\n",
            "Converting format of dataset: 100% 2415/2415 [00:00<00:00, 74043.64 examples/s]\n",
            "Running tokenizer on dataset: 100% 2415/2415 [00:00<00:00, 2582.23 examples/s]\n",
            "input_ids:\n",
            "[1, 733, 16289, 28793, 17824, 456, 12271, 297, 281, 14295, 3445, 13, 28777, 495, 528, 264, 1274, 302, 25399, 369, 506, 272, 500, 28726, 2794, 28718, 3469, 390, 652, 14014, 271, 28723, 733, 28748, 16289, 28793, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
            "inputs:\n",
            "<s> [INST] translate this sentence in docker command\n",
            "Give me a list of containers that have the Ubuntu image as their ancestor. [/INST] docker ps --filter 'ancestor=ubuntu'</s>\n",
            "label_ids:\n",
            "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 281, 14295, 12384, 1939, 4650, 464, 834, 374, 271, 28746, 437, 2794, 28718, 28742, 2]\n",
            "labels:\n",
            " docker ps --filter 'ancestor=ubuntu'</s>\n",
            "config.json: 100% 571/571 [00:00<00:00, 2.91MB/s]\n",
            "[INFO|configuration_utils.py:728] 2024-03-27 08:07:50,266 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-03-27 08:07:50,269 >> Model config MistralConfig {\n",
            "  \"_name_or_path\": \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "03/27/2024 08:07:50 - INFO - llmtuner.model.patcher - Quantizing model to 4 bit.\n",
            "model.safetensors.index.json: 100% 25.1k/25.1k [00:00<00:00, 78.5MB/s]\n",
            "[INFO|modeling_utils.py:3257] 2024-03-27 08:07:50,775 >> loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/model.safetensors.index.json\n",
            "Downloading shards:   0% 0/2 [00:00<?, ?it/s]\n",
            "model-00001-of-00002.safetensors:   0% 0.00/9.94G [00:00<?, ?B/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   0% 31.5M/9.94G [00:00<00:34, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 73.4M/9.94G [00:00<00:31, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   1% 115M/9.94G [00:00<00:30, 325MB/s] \u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 157M/9.94G [00:00<00:29, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 199M/9.94G [00:00<00:29, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   2% 241M/9.94G [00:00<00:29, 331MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 283M/9.94G [00:00<00:28, 333MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   3% 325M/9.94G [00:00<00:28, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 367M/9.94G [00:01<00:28, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   4% 409M/9.94G [00:01<00:28, 335MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 451M/9.94G [00:01<00:28, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 493M/9.94G [00:01<00:28, 337MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   5% 535M/9.94G [00:01<00:28, 336MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 577M/9.94G [00:01<00:28, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   6% 619M/9.94G [00:01<00:28, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 661M/9.94G [00:02<00:28, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 703M/9.94G [00:02<00:28, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   7% 744M/9.94G [00:02<00:29, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 786M/9.94G [00:02<00:30, 304MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   8% 818M/9.94G [00:02<00:30, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 849M/9.94G [00:02<00:30, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 881M/9.94G [00:02<00:31, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 912M/9.94G [00:02<00:31, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:   9% 944M/9.94G [00:02<00:31, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 975M/9.94G [00:03<00:30, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  10% 1.02G/9.94G [00:03<00:29, 304MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.06G/9.94G [00:03<00:28, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.10G/9.94G [00:03<00:27, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  11% 1.14G/9.94G [00:03<00:27, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.18G/9.94G [00:03<00:27, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  12% 1.23G/9.94G [00:03<00:26, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.27G/9.94G [00:03<00:26, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  13% 1.31G/9.94G [00:04<00:26, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.35G/9.94G [00:04<00:26, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.39G/9.94G [00:04<00:26, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  14% 1.44G/9.94G [00:04<00:25, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.48G/9.94G [00:04<00:25, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  15% 1.52G/9.94G [00:04<00:25, 332MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.56G/9.94G [00:04<00:25, 334MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  16% 1.60G/9.94G [00:04<00:25, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.65G/9.94G [00:05<00:25, 329MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.69G/9.94G [00:05<00:25, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  17% 1.73G/9.94G [00:05<00:25, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.77G/9.94G [00:05<00:25, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  18% 1.81G/9.94G [00:05<00:26, 311MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.85G/9.94G [00:05<00:26, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.88G/9.94G [00:05<00:26, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  19% 1.91G/9.94G [00:05<00:26, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.94G/9.94G [00:06<00:27, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 1.97G/9.94G [00:06<00:27, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.00G/9.94G [00:06<00:28, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  20% 2.03G/9.94G [00:06<00:28, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.07G/9.94G [00:06<00:28, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.10G/9.94G [00:06<00:27, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  21% 2.13G/9.94G [00:06<00:27, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.16G/9.94G [00:06<00:26, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.19G/9.94G [00:06<00:26, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  22% 2.22G/9.94G [00:07<00:26, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.25G/9.94G [00:07<00:26, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.29G/9.94G [00:07<00:27, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  23% 2.32G/9.94G [00:07<00:27, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.35G/9.94G [00:07<00:27, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.38G/9.94G [00:07<00:26, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  24% 2.41G/9.94G [00:07<00:26, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.45G/9.94G [00:07<00:25, 299MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  25% 2.50G/9.94G [00:08<00:24, 307MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.54G/9.94G [00:08<00:23, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.58G/9.94G [00:08<00:23, 315MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  26% 2.62G/9.94G [00:08<00:23, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.66G/9.94G [00:08<00:22, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  27% 2.71G/9.94G [00:08<00:22, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.75G/9.94G [00:08<00:22, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.79G/9.94G [00:08<00:22, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  28% 2.83G/9.94G [00:09<00:21, 327MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.87G/9.94G [00:09<00:21, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  29% 2.92G/9.94G [00:09<00:21, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 2.96G/9.94G [00:09<00:21, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  30% 3.00G/9.94G [00:09<00:21, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.04G/9.94G [00:09<00:21, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.08G/9.94G [00:09<00:21, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  31% 3.12G/9.94G [00:09<00:21, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.17G/9.94G [00:10<00:21, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  32% 3.21G/9.94G [00:10<00:21, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.24G/9.94G [00:10<00:22, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.27G/9.94G [00:10<00:22, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  33% 3.30G/9.94G [00:10<00:22, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.33G/9.94G [00:10<00:22, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.37G/9.94G [00:10<00:22, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.40G/9.94G [00:10<00:22, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  34% 3.43G/9.94G [00:11<00:22, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.46G/9.94G [00:11<00:21, 295MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.49G/9.94G [00:11<00:22, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  35% 3.52G/9.94G [00:11<00:22, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.55G/9.94G [00:11<00:23, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.59G/9.94G [00:11<00:23, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  36% 3.62G/9.94G [00:11<00:22, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.65G/9.94G [00:11<00:22, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.68G/9.94G [00:11<00:22, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  37% 3.71G/9.94G [00:12<00:21, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.74G/9.94G [00:12<00:23, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.77G/9.94G [00:12<00:22, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  38% 3.81G/9.94G [00:12<00:22, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.84G/9.94G [00:12<00:21, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.87G/9.94G [00:12<00:21, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  39% 3.90G/9.94G [00:12<00:20, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.94G/9.94G [00:12<00:19, 304MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 3.98G/9.94G [00:12<00:19, 308MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  40% 4.02G/9.94G [00:13<00:19, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.06G/9.94G [00:13<00:18, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  41% 4.10G/9.94G [00:13<00:18, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.14G/9.94G [00:13<00:18, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.18G/9.94G [00:13<00:17, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  42% 4.23G/9.94G [00:13<00:17, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.27G/9.94G [00:13<00:17, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  43% 4.31G/9.94G [00:13<00:17, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.35G/9.94G [00:14<00:17, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  44% 4.39G/9.94G [00:14<00:17, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.44G/9.94G [00:14<00:16, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.48G/9.94G [00:14<00:17, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  45% 4.52G/9.94G [00:14<00:17, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.56G/9.94G [00:14<00:17, 313MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  46% 4.60G/9.94G [00:14<00:16, 315MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.65G/9.94G [00:15<00:20, 255MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.68G/9.94G [00:15<00:21, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  47% 4.71G/9.94G [00:15<00:20, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.74G/9.94G [00:15<00:20, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.77G/9.94G [00:15<00:20, 256MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  48% 4.80G/9.94G [00:15<00:19, 262MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.83G/9.94G [00:15<00:19, 267MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.87G/9.94G [00:15<00:19, 266MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  49% 4.90G/9.94G [00:16<00:18, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.93G/9.94G [00:16<00:18, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.96G/9.94G [00:16<00:18, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  50% 4.99G/9.94G [00:16<00:17, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.02G/9.94G [00:16<00:17, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.05G/9.94G [00:16<00:17, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.09G/9.94G [00:16<00:17, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  51% 5.12G/9.94G [00:16<00:17, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.15G/9.94G [00:16<00:17, 271MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.18G/9.94G [00:17<00:17, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  52% 5.21G/9.94G [00:17<00:17, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.24G/9.94G [00:17<00:17, 269MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.27G/9.94G [00:17<00:17, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  53% 5.31G/9.94G [00:17<00:16, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.34G/9.94G [00:17<00:16, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.37G/9.94G [00:17<00:15, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  54% 5.41G/9.94G [00:17<00:15, 302MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.45G/9.94G [00:18<00:14, 309MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  55% 5.49G/9.94G [00:18<00:14, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.54G/9.94G [00:18<00:13, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  56% 5.58G/9.94G [00:18<00:13, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.62G/9.94G [00:18<00:13, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.66G/9.94G [00:18<00:13, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  57% 5.70G/9.94G [00:18<00:13, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.75G/9.94G [00:18<00:12, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  58% 5.79G/9.94G [00:19<00:12, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.83G/9.94G [00:19<00:12, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.87G/9.94G [00:19<00:12, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  59% 5.91G/9.94G [00:19<00:12, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 5.96G/9.94G [00:19<00:12, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  60% 6.00G/9.94G [00:19<00:12, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.04G/9.94G [00:19<00:12, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  61% 6.08G/9.94G [00:19<00:12, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.12G/9.94G [00:20<00:12, 300MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.16G/9.94G [00:20<00:12, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  62% 6.19G/9.94G [00:20<00:13, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.22G/9.94G [00:20<00:12, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.25G/9.94G [00:20<00:12, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.28G/9.94G [00:20<00:13, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  63% 6.31G/9.94G [00:20<00:12, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.34G/9.94G [00:20<00:12, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.38G/9.94G [00:21<00:12, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  64% 6.41G/9.94G [00:21<00:12, 282MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.44G/9.94G [00:21<00:12, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.47G/9.94G [00:21<00:11, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  65% 6.50G/9.94G [00:21<00:11, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.53G/9.94G [00:21<00:11, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.56G/9.94G [00:21<00:11, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  66% 6.60G/9.94G [00:21<00:11, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.63G/9.94G [00:21<00:11, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.66G/9.94G [00:22<00:11, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  67% 6.69G/9.94G [00:22<00:11, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.72G/9.94G [00:22<00:11, 275MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.75G/9.94G [00:22<00:11, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  68% 6.79G/9.94G [00:22<00:10, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.84G/9.94G [00:22<00:10, 303MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  69% 6.88G/9.94G [00:22<00:09, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.92G/9.94G [00:22<00:09, 315MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 6.96G/9.94G [00:23<00:09, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  70% 7.00G/9.94G [00:23<00:09, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.05G/9.94G [00:23<00:08, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  71% 7.09G/9.94G [00:23<00:08, 324MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.13G/9.94G [00:23<00:08, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  72% 7.17G/9.94G [00:23<00:08, 328MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.21G/9.94G [00:23<00:08, 330MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.26G/9.94G [00:23<00:08, 318MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  73% 7.30G/9.94G [00:24<00:08, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.34G/9.94G [00:24<00:08, 316MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  74% 7.38G/9.94G [00:24<00:08, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.42G/9.94G [00:24<00:07, 319MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  75% 7.47G/9.94G [00:24<00:07, 320MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.51G/9.94G [00:24<00:07, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.55G/9.94G [00:24<00:07, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  76% 7.59G/9.94G [00:25<00:10, 221MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.62G/9.94G [00:25<00:10, 215MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.65G/9.94G [00:25<00:10, 216MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  77% 7.69G/9.94G [00:25<00:09, 232MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.72G/9.94G [00:25<00:08, 249MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.76G/9.94G [00:25<00:08, 268MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  78% 7.80G/9.94G [00:25<00:07, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.84G/9.94G [00:26<00:07, 294MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  79% 7.87G/9.94G [00:26<00:07, 291MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.91G/9.94G [00:26<00:07, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.94G/9.94G [00:26<00:07, 280MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 7.97G/9.94G [00:26<00:06, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  80% 8.00G/9.94G [00:26<00:07, 277MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.03G/9.94G [00:26<00:06, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.06G/9.94G [00:26<00:06, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  81% 8.10G/9.94G [00:26<00:06, 276MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.13G/9.94G [00:27<00:06, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.16G/9.94G [00:27<00:06, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  82% 8.19G/9.94G [00:27<00:06, 252MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.22G/9.94G [00:27<00:07, 245MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.25G/9.94G [00:27<00:06, 251MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  83% 8.28G/9.94G [00:27<00:06, 257MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.32G/9.94G [00:27<00:06, 260MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.35G/9.94G [00:27<00:05, 272MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  84% 8.38G/9.94G [00:28<00:05, 283MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.42G/9.94G [00:28<00:05, 298MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  85% 8.46G/9.94G [00:28<00:04, 305MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.50G/9.94G [00:28<00:04, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.55G/9.94G [00:28<00:04, 314MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  86% 8.59G/9.94G [00:28<00:04, 317MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.63G/9.94G [00:28<00:04, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  87% 8.67G/9.94G [00:28<00:03, 322MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.71G/9.94G [00:29<00:03, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.76G/9.94G [00:29<00:03, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  88% 8.80G/9.94G [00:29<00:03, 326MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.84G/9.94G [00:29<00:03, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  89% 8.88G/9.94G [00:29<00:03, 325MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.92G/9.94G [00:29<00:03, 323MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  90% 8.97G/9.94G [00:29<00:03, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.01G/9.94G [00:30<00:02, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.05G/9.94G [00:30<00:02, 321MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  91% 9.09G/9.94G [00:30<00:02, 312MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.12G/9.94G [00:30<00:02, 310MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.15G/9.94G [00:30<00:02, 298MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  92% 9.19G/9.94G [00:30<00:02, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.22G/9.94G [00:30<00:02, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.25G/9.94G [00:30<00:02, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  93% 9.28G/9.94G [00:30<00:02, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.31G/9.94G [00:31<00:02, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.34G/9.94G [00:31<00:02, 289MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  94% 9.37G/9.94G [00:31<00:01, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.41G/9.94G [00:31<00:01, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.44G/9.94G [00:31<00:01, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  95% 9.47G/9.94G [00:31<00:01, 292MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.50G/9.94G [00:31<00:01, 293MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.53G/9.94G [00:31<00:01, 290MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.56G/9.94G [00:31<00:01, 286MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  96% 9.59G/9.94G [00:32<00:01, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.63G/9.94G [00:32<00:01, 287MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.66G/9.94G [00:32<00:01, 285MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  97% 9.69G/9.94G [00:32<00:00, 281MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.72G/9.94G [00:32<00:00, 278MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.75G/9.94G [00:32<00:00, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  98% 9.78G/9.94G [00:32<00:00, 273MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.81G/9.94G [00:32<00:00, 274MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.85G/9.94G [00:32<00:00, 279MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors:  99% 9.88G/9.94G [00:33<00:00, 284MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.91G/9.94G [00:33<00:00, 288MB/s]\u001b[A\n",
            "model-00001-of-00002.safetensors: 100% 9.94G/9.94G [00:33<00:00, 299MB/s]\n",
            "Downloading shards:  50% 1/2 [00:33<00:33, 33.40s/it]\n",
            "model-00002-of-00002.safetensors:   0% 0.00/4.54G [00:00<?, ?B/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   1% 41.9M/4.54G [00:00<00:13, 333MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   2% 83.9M/4.54G [00:00<00:13, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   3% 126M/4.54G [00:00<00:13, 337MB/s] \u001b[A\n",
            "model-00002-of-00002.safetensors:   4% 168M/4.54G [00:00<00:12, 337MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   5% 210M/4.54G [00:00<00:12, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 252M/4.54G [00:00<00:12, 338MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   6% 294M/4.54G [00:00<00:12, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   7% 336M/4.54G [00:00<00:12, 335MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   8% 377M/4.54G [00:01<00:12, 337MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:   9% 419M/4.54G [00:01<00:12, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  10% 461M/4.54G [00:01<00:12, 336MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  11% 503M/4.54G [00:01<00:11, 337MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  12% 545M/4.54G [00:01<00:11, 334MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  13% 587M/4.54G [00:01<00:11, 333MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  14% 629M/4.54G [00:01<00:11, 333MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  15% 671M/4.54G [00:02<00:11, 329MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  16% 713M/4.54G [00:02<00:11, 330MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 755M/4.54G [00:02<00:12, 310MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  17% 786M/4.54G [00:02<00:12, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  18% 818M/4.54G [00:02<00:12, 290MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 849M/4.54G [00:02<00:12, 292MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  19% 881M/4.54G [00:02<00:12, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  20% 912M/4.54G [00:02<00:12, 290MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 944M/4.54G [00:02<00:12, 286MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  21% 975M/4.54G [00:03<00:12, 285MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  22% 1.01G/4.54G [00:03<00:12, 287MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  23% 1.04G/4.54G [00:03<00:12, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.07G/4.54G [00:03<00:11, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  24% 1.10G/4.54G [00:03<00:12, 279MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  25% 1.13G/4.54G [00:03<00:12, 282MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.16G/4.54G [00:03<00:11, 282MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  26% 1.20G/4.54G [00:03<00:11, 285MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  27% 1.23G/4.54G [00:03<00:11, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.26G/4.54G [00:04<00:11, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  28% 1.29G/4.54G [00:04<00:11, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  29% 1.32G/4.54G [00:04<00:17, 186MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  30% 1.36G/4.54G [00:04<00:14, 221MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  31% 1.41G/4.54G [00:04<00:12, 248MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  32% 1.45G/4.54G [00:04<00:11, 270MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  33% 1.49G/4.54G [00:04<00:10, 287MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  34% 1.53G/4.54G [00:05<00:10, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  35% 1.57G/4.54G [00:05<00:09, 311MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.61G/4.54G [00:05<00:09, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  36% 1.66G/4.54G [00:05<00:09, 319MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  37% 1.70G/4.54G [00:05<00:08, 323MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  38% 1.74G/4.54G [00:05<00:08, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  39% 1.78G/4.54G [00:05<00:08, 328MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  40% 1.82G/4.54G [00:05<00:08, 326MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  41% 1.87G/4.54G [00:06<00:08, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  42% 1.91G/4.54G [00:06<00:08, 326MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  43% 1.95G/4.54G [00:06<00:07, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  44% 1.99G/4.54G [00:06<00:08, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.03G/4.54G [00:06<00:08, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  45% 2.07G/4.54G [00:06<00:08, 298MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  46% 2.10G/4.54G [00:06<00:08, 283MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  47% 2.13G/4.54G [00:07<00:08, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.16G/4.54G [00:07<00:08, 287MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  48% 2.19G/4.54G [00:07<00:08, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  49% 2.22G/4.54G [00:07<00:08, 274MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.25G/4.54G [00:07<00:08, 274MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  50% 2.29G/4.54G [00:07<00:08, 271MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  51% 2.32G/4.54G [00:07<00:07, 278MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.35G/4.54G [00:07<00:07, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  52% 2.38G/4.54G [00:07<00:07, 284MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  53% 2.41G/4.54G [00:08<00:07, 288MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  54% 2.44G/4.54G [00:08<00:07, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.47G/4.54G [00:08<00:07, 293MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  55% 2.51G/4.54G [00:08<00:06, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  56% 2.54G/4.54G [00:08<00:06, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.57G/4.54G [00:08<00:06, 297MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  57% 2.60G/4.54G [00:08<00:06, 295MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  58% 2.63G/4.54G [00:08<00:06, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.66G/4.54G [00:08<00:06, 299MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  59% 2.69G/4.54G [00:08<00:06, 303MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  60% 2.73G/4.54G [00:09<00:06, 300MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  61% 2.77G/4.54G [00:09<00:05, 302MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  62% 2.80G/4.54G [00:09<00:05, 296MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  63% 2.84G/4.54G [00:09<00:05, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.88G/4.54G [00:09<00:05, 311MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  64% 2.93G/4.54G [00:09<00:05, 316MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  65% 2.97G/4.54G [00:09<00:04, 318MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  66% 3.01G/4.54G [00:09<00:04, 319MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  67% 3.05G/4.54G [00:10<00:05, 270MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  68% 3.08G/4.54G [00:10<00:05, 278MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  69% 3.12G/4.54G [00:10<00:04, 292MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.17G/4.54G [00:10<00:04, 300MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  70% 3.20G/4.54G [00:10<00:04, 303MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  71% 3.23G/4.54G [00:10<00:04, 306MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  72% 3.26G/4.54G [00:10<00:04, 307MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  73% 3.30G/4.54G [00:10<00:03, 312MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  74% 3.34G/4.54G [00:11<00:03, 317MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  75% 3.39G/4.54G [00:11<00:03, 320MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.43G/4.54G [00:11<00:03, 322MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  76% 3.47G/4.54G [00:11<00:03, 322MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  77% 3.51G/4.54G [00:11<00:03, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  78% 3.55G/4.54G [00:11<00:03, 325MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  79% 3.60G/4.54G [00:11<00:02, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  80% 3.64G/4.54G [00:12<00:02, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  81% 3.68G/4.54G [00:12<00:02, 328MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  82% 3.72G/4.54G [00:12<00:02, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  83% 3.76G/4.54G [00:12<00:02, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  84% 3.81G/4.54G [00:12<00:02, 327MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  85% 3.85G/4.54G [00:12<00:02, 323MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.89G/4.54G [00:12<00:02, 314MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  86% 3.92G/4.54G [00:12<00:02, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  87% 3.95G/4.54G [00:13<00:01, 308MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 3.98G/4.54G [00:13<00:01, 304MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  88% 4.02G/4.54G [00:13<00:01, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  89% 4.05G/4.54G [00:13<00:01, 294MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  90% 4.08G/4.54G [00:13<00:01, 290MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.11G/4.54G [00:13<00:01, 294MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  91% 4.14G/4.54G [00:13<00:01, 296MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  92% 4.17G/4.54G [00:13<00:01, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.20G/4.54G [00:13<00:01, 301MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  93% 4.24G/4.54G [00:13<00:01, 288MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  94% 4.27G/4.54G [00:14<00:00, 289MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.30G/4.54G [00:14<00:00, 287MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  95% 4.33G/4.54G [00:14<00:00, 294MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  96% 4.36G/4.54G [00:14<00:00, 291MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.39G/4.54G [00:14<00:00, 282MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  97% 4.42G/4.54G [00:14<00:00, 280MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  98% 4.46G/4.54G [00:14<00:00, 268MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors:  99% 4.49G/4.54G [00:14<00:00, 266MB/s]\u001b[A\n",
            "model-00002-of-00002.safetensors: 100% 4.54G/4.54G [00:15<00:00, 301MB/s]\n",
            "Downloading shards: 100% 2/2 [00:48<00:00, 24.29s/it]\n",
            "[INFO|modeling_utils.py:1400] 2024-03-27 08:08:39,353 >> Instantiating MistralForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:845] 2024-03-27 08:08:39,355 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "Loading checkpoint shards: 100% 2/2 [00:09<00:00,  4.90s/it]\n",
            "[INFO|modeling_utils.py:3992] 2024-03-27 08:08:50,268 >> All model checkpoint weights were used when initializing MistralForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:4000] 2024-03-27 08:08:50,268 >> All the weights of MistralForCausalLM were initialized from the model checkpoint at mistralai/Mistral-7B-Instruct-v0.1.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MistralForCausalLM for predictions without further training.\n",
            "generation_config.json: 100% 116/116 [00:00<00:00, 594kB/s]\n",
            "[INFO|configuration_utils.py:800] 2024-03-27 08:08:50,446 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/generation_config.json\n",
            "[INFO|configuration_utils.py:845] 2024-03-27 08:08:50,446 >> Generate config GenerationConfig {\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2\n",
            "}\n",
            "\n",
            "03/27/2024 08:08:50 - INFO - llmtuner.model.patcher - Gradient checkpointing enabled.\n",
            "03/27/2024 08:08:50 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "03/27/2024 08:08:50 - INFO - llmtuner.model.loader - trainable params: 3407872 || all params: 7245139968 || trainable%: 0.0470\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n",
            "[INFO|trainer.py:601] 2024-03-27 08:08:50,922 >> Using auto half precision backend\n",
            "[INFO|trainer.py:1812] 2024-03-27 08:08:51,274 >> ***** Running training *****\n",
            "[INFO|trainer.py:1813] 2024-03-27 08:08:51,274 >>   Num examples = 2,415\n",
            "[INFO|trainer.py:1814] 2024-03-27 08:08:51,274 >>   Num Epochs = 1\n",
            "[INFO|trainer.py:1815] 2024-03-27 08:08:51,274 >>   Instantaneous batch size per device = 16\n",
            "[INFO|trainer.py:1818] 2024-03-27 08:08:51,274 >>   Total train batch size (w. parallel, distributed & accumulation) = 64\n",
            "[INFO|trainer.py:1819] 2024-03-27 08:08:51,274 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1820] 2024-03-27 08:08:51,274 >>   Total optimization steps = 37\n",
            "[INFO|trainer.py:1821] 2024-03-27 08:08:51,277 >>   Number of trainable parameters = 3,407,872\n",
            "03/27/2024 08:09:11 - INFO - llmtuner.extras.callbacks - {'loss': 0.6566, 'learning_rate': 1.9112e-04, 'epoch': 0.13}\n",
            "{'loss': 0.6566, 'grad_norm': 0.9974868297576904, 'learning_rate': 0.0001911228490388136, 'epoch': 0.13}\n",
            "03/27/2024 08:09:29 - INFO - llmtuner.extras.callbacks - {'loss': 0.3699, 'learning_rate': 1.6607e-04, 'epoch': 0.26}\n",
            "{'loss': 0.3699, 'grad_norm': 1.1022095680236816, 'learning_rate': 0.00016606747233900815, 'epoch': 0.26}\n",
            "03/27/2024 08:09:48 - INFO - llmtuner.extras.callbacks - {'loss': 0.2073, 'learning_rate': 1.2928e-04, 'epoch': 0.40}\n",
            "{'loss': 0.2073, 'grad_norm': 0.8443474173545837, 'learning_rate': 0.00012928227712765504, 'epoch': 0.4}\n",
            "03/27/2024 08:10:06 - INFO - llmtuner.extras.callbacks - {'loss': 0.1801, 'learning_rate': 8.7298e-05, 'epoch': 0.53}\n",
            "{'loss': 0.1801, 'grad_norm': 1.855509638786316, 'learning_rate': 8.729821802531212e-05, 'epoch': 0.53}\n",
            "03/27/2024 08:10:25 - INFO - llmtuner.extras.callbacks - {'loss': 0.1124, 'learning_rate': 4.7569e-05, 'epoch': 0.66}\n",
            "{'loss': 0.1124, 'grad_norm': 1.2832125425338745, 'learning_rate': 4.756927164427685e-05, 'epoch': 0.66}\n",
            "03/27/2024 08:10:43 - INFO - llmtuner.extras.callbacks - {'loss': 0.1248, 'learning_rate': 1.7149e-05, 'epoch': 0.79}\n",
            "{'loss': 0.1248, 'grad_norm': 1.0267609357833862, 'learning_rate': 1.7149035075615794e-05, 'epoch': 0.79}\n",
            "03/27/2024 08:11:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.1097, 'learning_rate': 1.4384e-06, 'epoch': 0.93}\n",
            "{'loss': 0.1097, 'grad_norm': 0.7090836763381958, 'learning_rate': 1.4384089652291543e-06, 'epoch': 0.93}\n",
            "[INFO|trainer.py:2067] 2024-03-27 08:11:10,842 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "03/27/2024 08:11:10 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 0.98}\n",
            "{'train_runtime': 139.5651, 'train_samples_per_second': 17.304, 'train_steps_per_second': 0.265, 'train_loss': 0.24330653975138794, 'epoch': 0.98}\n",
            "[INFO|trainer.py:3067] 2024-03-27 08:11:10,844 >> Saving model checkpoint to saves/Mistral-7B-Chat/lora/train_2024-03-27-08-00-25\n",
            "[INFO|configuration_utils.py:728] 2024-03-27 08:11:11,061 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--mistralai--Mistral-7B-Instruct-v0.1/snapshots/73068f3702d050a2fd5aa2ca1e612e5036429398/config.json\n",
            "[INFO|configuration_utils.py:791] 2024-03-27 08:11:11,062 >> Model config MistralConfig {\n",
            "  \"architectures\": [\n",
            "    \"MistralForCausalLM\"\n",
            "  ],\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 1,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 32768,\n",
            "  \"model_type\": \"mistral\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_theta\": 10000.0,\n",
            "  \"sliding_window\": 4096,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 32000\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2459] 2024-03-27 08:11:11,093 >> tokenizer config file saved in saves/Mistral-7B-Chat/lora/train_2024-03-27-08-00-25/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2468] 2024-03-27 08:11:11,093 >> Special tokens file saved in saves/Mistral-7B-Chat/lora/train_2024-03-27-08-00-25/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       0.98\n",
            "  train_loss               =     0.2433\n",
            "  train_runtime            = 0:02:19.56\n",
            "  train_samples_per_second =     17.304\n",
            "  train_steps_per_second   =      0.265\n",
            "[INFO|modelcard.py:450] 2024-03-27 08:11:11,097 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4NqFRBye4jcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cE7w9yRVwAzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa-IrJS1aRVJ"
      },
      "source": [
        "In order to use a GPU with your notebook, select the `Runtime > Change runtime type` menu, and then set the hardware accelerator dropdown to GPU."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65MSuHKqNeBZ"
      },
      "source": [
        "## More memory\n",
        "\n",
        "Users who have purchased one of Colab's paid plans have access to high-memory VMs when they are available.\n",
        "\n",
        "\n",
        "\n",
        "You can see how much memory you have available at any time by running the following code cell. If the execution result of running the code cell below is \"Not using a high-RAM runtime\", then you can enable a high-RAM runtime via `Runtime > Change runtime type` in the menu. Then select High-RAM in the Runtime shape dropdown. After, re-execute the code cell.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V1G82GuO-tez"
      },
      "outputs": [],
      "source": [
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('Not using a high-RAM runtime')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJW8Qi-pPpep"
      },
      "source": [
        "## Longer runtimes\n",
        "\n",
        "All Colab runtimes are reset after some period of time (which is faster if the runtime isn't executing code). Colab Pro and Pro+ users have access to longer runtimes than those who use Colab free of charge.\n",
        "\n",
        "## Background execution\n",
        "\n",
        "Colab Pro+ users have access to background execution, where notebooks will continue executing even after you've closed a browser tab. This is always enabled in Pro+ runtimes as long as you have compute units available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLlTRcMM_h0k"
      },
      "source": [
        "## Relaxing resource limits in Colab Pro\n",
        "\n",
        "Your resources are not unlimited in Colab. To make the most of Colab, avoid using resources when you don't need them. For example, only use a GPU when required and close Colab tabs when finished.\n",
        "\n",
        "\n",
        "\n",
        "If you encounter limitations, you can relax those limitations by purchasing more compute units via Pay As You Go. Anyone can purchase compute units via [Pay As You Go](https://colab.research.google.com/signup); no subscription is required."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mm8FzEidvPs6"
      },
      "source": [
        "## Send us feedback!\n",
        "\n",
        "If you have any feedback for us, please let us know. The best way to send feedback is by using the Help > 'Send feedback...' menu. If you encounter usage limits in Colab Pro consider subscribing to Pro+.\n",
        "\n",
        "If you encounter errors or other issues with billing (payments) for Colab Pro, Pro+, or Pay As You Go, please email [colab-billing@google.com](mailto:colab-billing@google.com)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qB3bdLe8jkAa"
      },
      "source": [
        "## More Resources\n",
        "\n",
        "### Working with Notebooks in Colab\n",
        "- [Overview of Colaboratory](/notebooks/basic_features_overview.ipynb)\n",
        "- [Guide to Markdown](/notebooks/markdown_guide.ipynb)\n",
        "- [Importing libraries and installing dependencies](/notebooks/snippets/importing_libraries.ipynb)\n",
        "- [Saving and loading notebooks in GitHub](https://colab.research.google.com/github/googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb)\n",
        "- [Interactive forms](/notebooks/forms.ipynb)\n",
        "- [Interactive widgets](/notebooks/widgets.ipynb)\n",
        "\n",
        "<a name=\"working-with-data\"></a>\n",
        "### Working with Data\n",
        "- [Loading data: Drive, Sheets, and Google Cloud Storage](/notebooks/io.ipynb)\n",
        "- [Charts: visualizing data](/notebooks/charts.ipynb)\n",
        "- [Getting started with BigQuery](/notebooks/bigquery.ipynb)\n",
        "\n",
        "### Machine Learning Crash Course\n",
        "These are a few of the notebooks from Google's online Machine Learning course. See the [full course website](https://developers.google.com/machine-learning/crash-course/) for more.\n",
        "- [Intro to Pandas DataFrame](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/pandas_dataframe_ultraquick_tutorial.ipynb)\n",
        "- [Linear regression with tf.keras using synthetic data](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb)\n",
        "\n",
        "\n",
        "<a name=\"using-accelerated-hardware\"></a>\n",
        "### Using Accelerated Hardware\n",
        "- [TensorFlow with GPUs](/notebooks/gpu.ipynb)\n",
        "- [TensorFlow with TPUs](/notebooks/tpu.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFm2S0Gijqo8"
      },
      "source": [
        "<a name=\"machine-learning-examples\"></a>\n",
        "\n",
        "## Machine Learning Examples\n",
        "\n",
        "To see end-to-end examples of the interactive machine learning analyses that Colaboratory makes possible, check out these  tutorials using models from [TensorFlow Hub](https://tfhub.dev).\n",
        "\n",
        "A few featured examples:\n",
        "\n",
        "- [Retraining an Image Classifier](https://tensorflow.org/hub/tutorials/tf2_image_retraining): Build a Keras model on top of a pre-trained image classifier to distinguish flowers.\n",
        "- [Text Classification](https://tensorflow.org/hub/tutorials/tf2_text_classification): Classify IMDB movie reviews as either *positive* or *negative*.\n",
        "- [Style Transfer](https://tensorflow.org/hub/tutorials/tf2_arbitrary_image_stylization): Use deep learning to transfer style between images.\n",
        "- [Multilingual Universal Sentence Encoder Q&A](https://tensorflow.org/hub/tutorials/retrieval_with_tf_hub_universal_encoder_qa): Use a machine learning model to answer questions from the SQuAD dataset.\n",
        "- [Video Interpolation](https://tensorflow.org/hub/tutorials/tweening_conv3d): Predict what happened in a video between the first and the last frame.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Making the Most of your Colab Subscription",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}